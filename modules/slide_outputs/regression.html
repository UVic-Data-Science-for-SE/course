<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="nernst">
<meta name="dcterms.date" content="2023-05-01">

<title>R and Data Mining Basics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="regression_files/libs/clipboard/clipboard.min.js"></script>
<script src="regression_files/libs/quarto-html/quarto.js"></script>
<script src="regression_files/libs/quarto-html/popper.min.js"></script>
<script src="regression_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="regression_files/libs/quarto-html/anchor.min.js"></script>
<link href="regression_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="regression_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="regression_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="regression_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="regression_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">R and Data Mining Basics</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>nernst </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 1, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>This is a <a href="https://quarto.org">Quarto</a>-based notebook. When you execute code within the notebook, the results appear beneath the code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.2     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.2     ✔ tibble    3.2.1
✔ lubridate 1.9.2     ✔ tidyr     1.3.0
✔ purrr     1.0.1     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>jm1 <span class="ot">=</span> <span class="fu">read.arff</span>(<span class="st">"http://promise.site.uottawa.ca/SERepository/datasets/jm1.arff"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>jm1 <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(jm1)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>jm1<span class="sc">$</span>defects <span class="ot">&lt;-</span> <span class="fu">as.logical</span>(jm1<span class="sc">$</span>defects)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="regression" class="level1">
<h1>Regression</h1>
<p>A useful approach, which we covered in Assignment 1, is to do a simple linear regression. In the simplest sense this is about finding the line that best fits the data. Extending the line to unseen datapoints is a <em>prediction</em> about what the y value should be for that data (a set of observations X).</p>
<section id="digression-modeling" class="level2">
<h2 class="anchored" data-anchor-id="digression-modeling">Digression: Modeling</h2>
<p>The word ‘model’ is overloaded in computer science (i.e., overused). Here, I will use it to mean <em>statistical model</em>, and it captures the intuition we discussed earlier: how we think the data are being generated. In this course I take the position that it is <em>usually</em> always possible to represent data with a simpler, more abstract model. For example, obviously how each individual in this course got to be the height they are is subject to a vast array of forces, including genetics (the big one), nutrition, air pollution, etc. In data science, we would like to abstract all of this complexity at the individual level with an abstraction for the sample, and talk about averages, deviations, and assume that randomness in sufficiently large numbers <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> will take care of the individual variation.</p>
<p>Another approach is to throw up our hands and forget about representing the data using an underlying generative model, and instead fit functions of high dimensionality to represent the way the data is distributed, the neural approach. We will see both in this course.</p>
</section>
<section id="ols-regression" class="level2">
<h2 class="anchored" data-anchor-id="ols-regression">OLS regression</h2>
<p>Ordinary least squares (OLS) regression is possible when there is a simple relationship to plot. It looks like: <span class="math inline">\(y_i = \beta x_i + \alpha + \epsilon_i\)</span> And our task is to find <span class="math inline">\(\alpha, \beta\)</span> to minimize the (squared) error in our estimated <span class="math inline">\(\hat{y}\)</span>, i.e.&nbsp;the value <span class="math inline">\(y_i - \hat{y}\)</span> for all the <span class="math inline">\(y\)</span> s. If we think about y as predicting individual height h, we can simply say your height is some intercept <span class="math inline">\(\alpha\)</span> and an error term. The error term <span class="math inline">\(\epsilon\)</span> here is capturing things like the inherent error in our measurements (if we are measuring 100 students, we might get sloppy).</p>
<p>This regression equation has a precise (algebraic) solution using the <a href="https://en.wikipedia.org/wiki/Least_squares"><em>ordinary least squares</em></a> technique.</p>
<p>An alternative representation uses the Gaussian model which captures the intuition that the residuals (difference between estimate and actual) are normally distributed about the mean:</p>
<p><span class="math inline">\(y_i \sim \mathcal{N}(\mu, \sigma)\)</span></p>
<p><span class="math inline">\(\mu \sim Normal(param, param)\)</span></p>
<p><span class="math inline">\(\sigma \sim\)</span> Uniform(param, param)</p>
<p>This equation says that our data point <span class="math inline">\(y_i\)</span> is modeled by (the <span class="math inline">\(\sim\)</span> or tilde operator) a Normal distribution with location (mean) <span class="math inline">\(\mu\)</span> and dispersion (variance) <span class="math inline">\(\sigma\)</span>. The next two lines are where we might specify our <strong>prior</strong> about how the location and dispersion should be defined.</p>
<p>The generalized version of this is a <em>generalized linear model</em> or GLM, and accounts for things like how the data and the marginals (the error) are distributed, as we saw in A1.</p>
<p>R has a built in function, <code>glm()</code> (for generalized linear model) that is frequentist:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fit_1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(defects <span class="sc">~</span> loc, <span class="at">data=</span>jm1, <span class="at">family=</span>binomial)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
</div>
</div>
<p>Let’s break this down a bit. The first part is the <code>defects ~ loc</code> notation. This is R’s way of encoding the statistical model. In this case, we are saying that the response in whether the row has <code>defect=True</code> is modeled by the <code>loc</code> (lines of code) feature (predictors). We could maybe write <code>defects ~ loc + v(g)</code>. These are two separate models; the two include linear predictors with no interactions. Special variants include <code>defects ~ .</code> (all predictors) and <code>defects ~ 1</code> (no predictors).</p>
<p>Finally, we have a parameter <code>family=binomial</code>. This parameter is saying that for this regression model, the sampler (in this case, a maximum likelihood estimation (MLE, a variant of Newton’s method)) should do logistic regression and predict a binary outcome. Both logistic and linear regression are variants of <em>generalized linear models</em>.</p>
</section>
</section>
<section id="interpreting-regression-results" class="level1">
<h1>Interpreting Regression Results</h1>
<p>(see also ROS, chapter 6.3, and chapter 10, p133)</p>
<p>We now have a model fit object <code>fit</code> that we will want to explore. Hopefully, we will see that it fits the equation for a linear model we mentioned above, with intercepts and coefficients:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glm(formula = defects ~ loc, family = binomial, data = jm1)

Coefficients:
(Intercept)          loc  
   -1.93604      0.01086  

Degrees of Freedom: 10884 Total (i.e. Null);  10883 Residual
Null Deviance:      10690 
Residual Deviance: 9961     AIC: 9965</code></pre>
</div>
</div>
<p>This prints out coefficient on the predictor (‘m’) and the intercept (that’s the <code>b</code> in the <code>y=mx + b + e</code> notation). The second column reflects the uncertainty in the estimate. So what do we make of this? The first thing we should do is print out one of the interactions:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(jm1<span class="sc">$</span>defects,jm1<span class="sc">$</span>loc)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>a_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit_1)[<span class="dv">1</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>b_hat <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit_1)[<span class="dv">2</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(a_hat, b_hat, <span class="at">col=</span><span class="st">'red'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="regression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Here we have our two values for defective files, and the LOC on the y-axis. A red line indicates the regression equation that fits this data using OLS. From our <code>fit_1</code> object, we can see the intercept is b=-1.9, and m is 0.01 (the slope of the red line). Recall our model: we think (or, we are testing), the idea that if code has more lines, it is more likely to have defects. So what this model predicts is that the change in lines of code increases ‘defect’ likelihood by 0.01 (1%) of the number of lines. This corresponds to a defect rate of 1 bug per 100 lines of code, which is actually pretty close to reports: Microsoft at one point estimated 10-20 defects per 1000 LOC <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>A few issues to note here:</p>
<ul>
<li>this is just one model, and if we follow our workflow, we should now be comparing this model to others, which might be changes to this one, or adding new features.</li>
<li>we have a negative intercept, which makes no sense because we can’t have negative LOC. We would want to fix that in our revised model.</li>
<li>we have a lot of data distributed around the 1-100 LOC mark. We should adjust our model (at the very least, our visualization) to accommodate this non-normal distribution.</li>
</ul>
</section>
<section id="bayesian-regression" class="level1">
<h1>Bayesian regression</h1>
<p>From a Bayesian point of view we can use the <code>rstanarm</code> library, which is the same format as the <code>glm()</code> command, but uses a Bayesian interpretation and sampler:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rstanarm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Rcpp</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>This is rstanarm version 2.21.4</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>- For execution on a local, multicore CPU with excess RAM we recommend calling</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>  options(mc.cores = parallel::detectCores())</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span>(<span class="at">mc.cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>fit_b <span class="ot">=</span> <span class="fu">stan_glm</span>(defects <span class="sc">~</span> loc , <span class="at">data =</span> jm1, <span class="at">family=</span>binomial)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(fit_b,<span class="at">digits=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>stan_glm
 family:       binomial [logit]
 formula:      defects ~ loc
 observations: 10885
 predictors:   2
------
            Median MAD_SD
(Intercept) -1.936  0.035
loc          0.011  0.000

------
* For help interpreting the printed output see ?print.stanreg
* For info on the priors used see ?prior_summary.stanreg</code></pre>
</div>
</div>
<p>Happily (I think you would agree) the Bayesian approach finds similar parameters as the frequentist/MLE approach before.</p>
<p>You might say “the outputs of both of these approaches are very similar” and thus, who cares if it is frequentist. Pragmatically I suppose that is true but you can get into trouble in the edges. In some sense we would hope that two different inferential approaches will find the <em>same</em> outcomes, if that outcome is a real thing.</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Bayes</th>
<th>Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prior captures past knowledge</td>
<td>Assumption of ignorance about past</td>
</tr>
<tr class="even">
<td>Sample from likelihood</td>
<td>Maximum likelihood estimation (MLE)</td>
</tr>
<tr class="odd">
<td>Computationally more costly</td>
<td>usually cheap</td>
</tr>
<tr class="even">
<td>Posterior represents probability of the parameter being true</td>
<td>p-value captures likelihood of observing data as extreme or more so, under null hypothesis</td>
</tr>
</tbody>
</table>
</section>
<section id="model-comparison-and-evaluation" class="level1">
<h1>Model Comparison and Evaluation</h1>
<p>For the frequentist regression approach, we have a metric such as <span class="math inline">\(R^2\)</span>, which measures the goodness of fit or proportion of variance explained. A perfect linear model would have <span class="math inline">\(R^2\)</span> of 1.0, indicating <em>all</em> the variance in the data is captured in the model. For our <code>fit_1</code> object above, this value is generalized as the difference between the null deviance (the model’s predictions) and the residual deviance (the amount remaining). Here, this value is 1 - (Residual Deviance/Null Deviance) or</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">=</span> <span class="dv">1</span> <span class="sc">-</span> (fit_1<span class="sc">$</span>deviance <span class="sc">/</span> fit_1<span class="sc">$</span>null.deviance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Having a Bayesian model is helpful in capturing an intuition about the data. It produces a fit object that is really a <em>posterior probability distribution</em> that gives us the ability to do stuff like sample from it, retrieve arbitrary intervals, etc.</p>
<p>One thing we might</p>
<section id="overfitting" class="level2">
<h2 class="anchored" data-anchor-id="overfitting">Overfitting</h2>
<p>One problem is that a model with 8 datapoints is perfectly explained by 8 variables. This overfitting issue can occur in regression as well, if we have models - like this jm1 dataset - with many variables. It also makes interpretation harder, as we have to explain the outputs based on more of these inputs. Thus we might penalize the regression model to prevent overfitting and reward simpler models. Two such approaches are <em>ridge</em> and <em>lasso</em> regression. In ridge regression we penalize the <span class="math inline">\(\beta\)</span> parameter above and seek to minimize the error AND the penalty; in lasso (least absolute shrinkage and selection operator) the penalty acts to see which coefficients can be forced to 0 (and discarded). This creates a sparser model; our error will probably be higher, but the explainability and generalizability should be increased.</p>
<p>Scikit-learn has a <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#linear-models-with-sparse-coefficients">nice Python explanation of this problem</a>.</p>
<hr>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1" role="doc-endnote"><p>what it means for a sample to be sufficiently large, of course, is an important question. If for example we have a class with a bunch of Dutch exchange students, even our class of 65 students will be too small to say much about UVic students in general.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>As reported in McConnell’s Code Complete, and quoted here: https://stackoverflow.com/questions/2898571/basis-for-claim-that-the-number-of-bugs-per-line-of-code-is-constant-regardless<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>
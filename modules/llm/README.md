---
author: Neil Ernst
title: Large Language Models and Natural Language Processing in SE
---

AI-supported development tools, like Codex, Copilot, ChatGPT, etc., have taken a big role in SE recently. What underpins these tools, how do they work so well, and what can we expect for SE in the AI future?

# Learning Outcomes

- a more than passing awareness of how large language models "work" on code
- ability to discuss the (current) tradeoffs of these tools
- analyze the way such tools are evaluated and discern hype from reality

| #     | Topic                   | Readings             | Exercises |
| ----- | ----------------------- | -------------------- | --------- |
| LLM-1 | [LLM overview](slides/llm-intro.md) â€¢ [Research Opportunities](slides/Grad-Opportunities.md)   | Naturalness paper |  [Self-Attention from Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)        |
| LLM-2 | Discussion and analysis |       Remaining 3 papers               |           |


## Required Readings

* Hindle et al., [On the Naturalness of Software](https://dl.acm.org/doi/10.5555/2337223.2337322)
* [Codex](https://arxiv.org/pdf/2107.03374.pdf)
* [StarCoder](https://arxiv.org/abs/2305.06161)
* [Patch Generation With Language Models: Feasibility and Scaling Behavior](https://openreview.net/pdf?id=rHlzJh_b1-5)

## Optional Readings and Activities

* Karampatsis, [Big Code != Big Vocabulary](https://ieeexplore.ieee.org/abstract/document/9284032) 
  Autocomplete.
* Xu, Vasilescu, Neubig, ["In IDE Code Generation from Natural Language"](https://arxiv.org/abs/2101.11149) [sections 1-4, 8,9]
* [LeGoues podcast audio](https://www.youtube.com/watch?v=YPfyRT80VJI)
* LeGoues, [Survey of APR](http://www.cs.cmu.edu/~clegoues/docs/legoues-cacm2019.pdf) 
* Alammar, [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* Vaswani et al. [Attention is all you need](https://arxiv.org/abs/1706.03762)